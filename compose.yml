services:
  redis:
    image: redis:${REDIS_VERSION}
    container_name: anilibrary-scraper-redis
    command: redis-server --requirepass "${REDIS_PASSWORD}"
    environment:
      REDIS_REPLICATION_MODE: master
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli -a $$REDIS_PASSWORD ping | grep PONG" ]
      interval: 5s
      timeout: 5s
      retries: 5
    ports:
      - ${REDIS_PORT}:6379
    volumes:
      - redis-data:/data
    networks:
      - local

  kafka:
    build:
      context: build/kafka
      args:
        KAFKA_VERSION: ${KAFKA_VERSION}
    container_name: anilibrary-scraper-kafka
    hostname: anilibrary-scraper-kafka
    env_file:
      - .env
    environment:
      # KRaft configuration
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_KRAFT_CLUSTER_ID=anilibrary-scraper
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      # Broker configuration
      - KAFKA_CFG_LISTENERS=SCRAPER_CLIENT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=SCRAPER_CLIENT://kafka:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=SCRAPER_CLIENT:SASL_PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=SCRAPER_CLIENT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL=SCRAM-SHA-512
      - KAFKA_NUM_PARTITIONS=2
      - KAFKA_CFG_REPLICA_LAG_TIME_MAX_MS=10000
    healthcheck:
      test: [ "CMD-SHELL", "kafka-broker-api-versions.sh --bootstrap-server localhost:9092 --command-config=/opt/bitnami/kafka/config/client.properties" ]
      interval: 5s
      timeout: 5s
      retries: 5
    ports:
      - ${KAFKA_PORT}:9092
    networks:
      - local

  clickhouse:
    depends_on:
      kafka:
        condition: service_healthy
    image: clickhouse/clickhouse-server:${CLICKHOUSE_VERSION}
    container_name: anilibrary-scraper-clickhouse
    env_file:
      - .env
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1
      interval: 5s
      timeout: 5s
      retries: 5
    ports:
      - ${CLICKHOUSE_HTTP_PORT}:8123
      - ${CLICKHOUSE_TCP_PORT}:9000
    volumes:
      - ./build/clickhouse/config.xml:/etc/clickhouse-server/config.d/config.xml
      - clickhouse-data:/var/lib/clickhouse
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    networks:
      - local

  filebeat:
    depends_on:
      app:
        condition: service_healthy
    image: elastic/filebeat:${FILEBEAT_VERSION}
    profiles:
      - filebeat
    user: root
    container_name: anilibrary-scraper-filebeat
    command:
      - -e
      - --strict.perms=false
    env_file:
      - .env
    volumes:
      - ./build/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers/:/var/lib/docker/containers:ro
    networks:
      - shared

  app:
    depends_on:
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      kafka:
        condition: service_healthy
    build:
      args:
        TIMEZONE: ${TIMEZONE:-Europe/Kiev}
      dockerfile: build/app/Dockerfile
      target: final
    labels:
      co.elastic.logs/enabled: true
      co.elastic.logs/json.keys_under_root: true
      co.elastic.logs/json.overwrite_keys: true
      co.elastic.logs/json.add_error_key: true
      co.elastic.logs/json.expand_keys: true
    container_name: anilibrary-scraper
    env_file:
      - .env
    ports:
      - ${HTTP_MAIN_PORT}:${HTTP_MAIN_PORT}
      - ${HTTP_MONITORING_PORT}:${HTTP_MONITORING_PORT}
    networks:
      - local
      - shared

networks:
  local:
    name: anilibrary-scraper-local
    driver: bridge
  shared:
    name: anilibrary
    external: true

volumes:
  redis-data:
    name: anilibrary-scraper-redis-data
  clickhouse-data:
    name: anilibrary-scraper-clickhouse-data
